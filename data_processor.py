# text preprocessing module
import re
import jieba
import jieba.posseg as pseg
from collections import defaultdict
import pandas as pd

class TextPreprocessor:
    def __init__(self):
        self.stopwords = set()
        self.custom_dict = []
        self.initailze_resources()

    """Initialize stopwords and custom dictionary"""
    def initailze_resources(self):
        jieba.initialize()
        self.add_food_dictionary()
        self.load_stopwords()

    """Add takeaway food-related terms"""
    def add_food_dictionary(self):
        food_words =  [
            ('éº»è¾£çƒ«', 'n'), ('é»„ç„–é¸¡', 'n'), ('æŠ«è¨', 'n'), ('æ±‰å ¡', 'n'), ('å¥¶èŒ¶', 'n'),
            ('ç‚¸é¸¡', 'n'), ('å¯¿å¸', 'n'), ('æ‹‰é¢', 'n'), ('ç›–é¥­', 'n'), ('çƒ§çƒ¤', 'n'),
            ('ç±³çº¿', 'n'), ('é¥ºå­', 'n'), ('ç‚’é¥­', 'n'), ('é¢æ¡', 'n'), ('å¿«é¤', 'n'),
            ('ä¾¿å½“', 'n'), ('ç”œå“', 'n'), ('å’–å•¡', 'n'), ('æœæ±', 'n'), ('è›‹ç³•', 'n')
        ]
        takeaway_words = [
            ('å¤–å–', 'n'), ('é…é€', 'vn'), ('éª‘æ‰‹', 'n'), ('æ‰“åŒ…', 'v'), ('é€é¤', 'v'),
            ('é€è¾¾', 'v'), ('å‡†æ—¶è¾¾', 'n'), ('è¶…æ—¶', 'v'), ('ä¿æ¸©', 'v'), ('åŒ…è£…è¢‹', 'n'),
            ('å¤–å–ç›’', 'n'), ('é…é€è´¹', 'n'), ('æ»¡å‡', 'n'), ('ä¼˜æƒ åˆ¸', 'n')
        ]
        sentiment_words = [
            ('è¶…å¥½åƒ', 'a'), ('å·¨éš¾åƒ', 'a'), ('äº”æ˜Ÿå¥½è¯„', 'n'), ('å·®è¯„', 'n'),
            ('ç»ç»å­', 'a'), ('yyds', 'a'), ('è¸©é›·', 'v'), ('æ‹”è‰', 'v'),
            ('ç§è‰', 'v'), ('å›è´­', 'v'), ('æ¨è', 'v'), ('é¿å‘', 'v')
        ]

        for word, flag in food_words + takeaway_words + sentiment_words:    # add to jieba dictionary
            jieba.add_word(word, freq=1000, tag=flag)
            self.custom_dict.append((word, flag))

    """Load stopwords from file"""
    def load_stopwords(self):
        basic_stopwords = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº',
            'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»',
            'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™', 'è¿™ä¸ª',
            'é‚£', 'é‚£ä¸ª', 'åœ¨', 'è¿˜', 'æˆ‘ä»¬', 'ä»–ä»¬', 'ä½ ä»¬', 'å¥¹', 'ä»–',
            'å®ƒ', 'å•Š', 'å“¦', 'å—¯', 'å‘¢', 'å§', 'å—', 'å•¦', 'å‘€', 'å“‡'
        }
        takeaway_stopwords = {
            'å¤–å–', 'è®¢å•', 'é…é€', 'å•†å®¶', 'ç”¨æˆ·', 'æ‰‹æœº', 'å¹³å°', 'app',
            'ç¾å›¢', 'é¥¿äº†ä¹ˆ', 'ç‚¹é¤', 'è®¢è´­', 'è´­ä¹°'
        }
        punctuation = {
            'ï¼Œ', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ã€', 'ï¼›', 'ï¼š', 'ã€Œ', 'ã€', 'ã€', 'ã€',
            'ï¼ˆ', 'ï¼‰', 'ã€Š', 'ã€‹', 'ã€', 'ã€‘', 'ï½›', 'ï½', 'â€”', 'ï½', 'Â·',
            'ï¼', 'ï¹', 'ï¹’', 'ï¹”', 'ï¹•', 'ï¹–', 'ï¹—', 'ï¼‚', 'ï¼ƒ', 'ï¼„', 'ï¼…',
            'ï¼†', 'ï¼‡', 'ï¼ˆ', 'ï¼‰', 'ï¼Š', 'ï¼‹', 'ï¼Œ', 'ï¼', 'ï¼', 'ï¼', 'ï¼š',
            'ï¼›', 'ï¼œ', 'ï¼', 'ï¼', 'ï¼Ÿ', 'ï¼ ', 'ï¼»', 'ï¼¼', 'ï¼½', 'ï¼¾', 'ï¼¿',
            'ï½€', 'ï½›', 'ï½œ', 'ï½', 'ï½'
        }
        self.stopwords = basic_stopwords.union(takeaway_stopwords).union(punctuation)   # combine all stopwords
    
    """Clean and preprocess text"""
    def clean_text(self, text):
        if pd.isna(text):
            return ""
        text = str(text).strip().lower()   # convert to string, strip whitespace, lowercase

        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)    # remove URLs
        text = re.sub(r'@\w+\s?', '', text)
        text = re.sub(r'#\w+#', '', text)
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\sï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š"'']', ' ', text)   # remove special characters
        text = re.sub(r'\s+', ' ', text)    # normalize whitespace
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)  # limit repeated characters to two
        return text.strip()
    
    """Segment text and remove stopwords"""
    def segment_text(self, text):
        if not text:
            return []
        words_with_pos = pseg.cut(text)

        filtered_words = []
        for word, flag in words_with_pos:
            if (
                word not in self.stopwords 
                and len(word) > 1   # filter single characters
                and re.search('[\u4e00-\u9fa5]', word)  # filter Chinese words
                and flag not in ['x', 'w', 'm']):  # filter non-informative POS tags
                filtered_words.append(word)

        return filtered_words

    """process text in DataFrame column"""
    def process_dataframe(self, df, text_column='review'):
        process_df = df.copy()  # avoid modifying original DataFrame
        process_df['cleaned_text'] = process_df[text_column].apply(self.clean_text) # clean text
        process_df['segmented_text'] = process_df['cleaned_text'].apply(self.segment_text)  # segment text
        process_df['processed_text'] = process_df['segmented_text'].apply(lambda x: ' '.join(x))  # join back to string
        process_df['text_length'] = process_df['processed_text'].apply(len)  # calculate text length
        process_df['word_count'] = process_df['segmented_text'].apply(len)  # calculate word count

        print("Text preprocessing completed.")
        print(f" original column: {text_column} ")
        print(" new columns added: cleaned_text, segmented_text, processed_text, text_length, word_count ")
        return process_df
    
    """analyze text statistics"""
    def analyze_text_statistics(self, df):
        stats = {
            'avg_text_length': df['text_length'].mean(),  
            'avg_word_count': df['word_count'].mean(),
            'min_text_length': df['text_length'].min(),
            'max_text_length': df['text_length'].max(),
            'total_words': df['word_count'].sum(),
            'unique_words': len(set([word for words in df['segmented_text'] for word in words]))
        }

        print("="*50)   # separator line
        print("Text Statistics:")
        for key, value in stats.items():
            print(f"{key}: {value:.2f}" if isinstance(value, float) else f"{key}: {value}")
        return stats
    
# test the TextPreprocessor class
if __name__ == "__main__":
    test_data = pd.DataFrame({
        'review': [
            "è¿™ä¸ªå¤–å–çœŸçš„è¶…å¥½åƒï¼å¼ºçƒˆæ¨èï¼http://example.com",
            "æœåŠ¡æ€åº¦å·®ï¼Œé€é¤è¶…æ—¶ï¼Œå·®è¯„ï¼",
            "åŒ…è£…è¿˜å¯ä»¥ï¼Œå°±æ˜¯å‘³é“ä¸€èˆ¬èˆ¬ã€‚",
            None,
            "æˆ‘è§‰å¾—è¿˜ä¸é”™ï¼Œä¸‹æ¬¡è¿˜ä¼šå†ç‚¹çš„ï¼ğŸ˜Š"
        ],
        'label': [1, 0, 1, 0, 1]
    })

    preprocessor = TextPreprocessor()
    processed_data = preprocessor.process_dataframe(test_data)

    print(processed_data[['review', 'processed_text', 'word_count']].head())
    

